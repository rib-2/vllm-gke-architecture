apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: nm-vllm
spec:
  annotations:
    prometheus.kserve.io/port: "8000"
    prometheus.kserve.io/path: "/metrics"
  supportedModelFormats:
    - name: huggingface
      version: "1"
      autoSelect: true
      priority: 1
  protocolVersions:
    - v2
    - v1
  containers:
    - name: kserve-container
      image: vllm/vllm-openai:v0.4.2
      ports:
        - containerPort: 8000
          protocol: TCP
      resources:
        requests:
          cpu: "1"
          memory: 2Gi
        limits:
          cpu: "1"
          memory: 2Gi
      readinessProbe:
        httpGet:
          path: /health
          port: 8000
        failureThreshold: 1
        periodSeconds: 10
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-3
  annotations:
    serving.kserve.io/enable-metric-aggregation: "true"
    serving.kserve.io/enable-prometheus-scraping: "true"
spec:
  predictor:
    scaleTarget: 8
    scaleMetric: concurrency
    minReplicas: 1
    maxReplicas: 2
    model:
      runtime: nm-vllm
      
      modelFormat:
        name: huggingface
      args:
        - "--model"
        - "/mnt/models/models/meta-llama/Meta-Llama-3-8B-Instruct"
        - "--served-model-name"
        - "meta-llama/Meta-Llama-3-8B-Instruct"
        - "--disable-log-requests"
      storageUri: "pvc://runtime-pvc"
      resources:
        limits:
          cpu: "6"
          memory: 12Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "6"
          memory: 12Gi
          nvidia.com/gpu: "1"
---
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: llama-3-monitor
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      serving.kserve.io/inferenceservice: llama-3
  podMetricsEndpoints:
  - port: user-port
    interval: 5s
